{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "3CDyGOzL8DB3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "4FH2JwPa2HPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Check Runtime Compatibility\n",
        "import torch\n",
        "\n",
        "# Check for NVIDIA GPU\n",
        "def check_nvidia_gpu():\n",
        "    cuda_available = torch.cuda.is_available()\n",
        "    if cuda_available:\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"NVIDIA GPU detected: {gpu_name}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"No NVIDIA GPU detected.\")\n",
        "        return False\n",
        "\n",
        "# Set a global variable for runtime compatibility\n",
        "correct_runtime = check_nvidia_gpu()\n",
        "\n",
        "if not correct_runtime:\n",
        "    print(\"Use a compatible runtime with an NVIDIA GPU.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HcQY2lG8c62D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Free Colab (or T4 GPU) Step 1:"
      ],
      "metadata": {
        "id": "3CDyGOzL8DB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Install Stable Audio Tools\n",
        "!pip install stable-audio-tools"
      ],
      "metadata": {
        "id": "ncigUPUb8LxM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU >= L4 Step 1:"
      ],
      "metadata": {
        "id": "Cb-p6Tv38Nyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcedFEPxNg0V",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown Install Stable Audio Tools for an L4 GPU Runtime on colab or greater - installs flas_attn\n",
        "!pip install flash_attn\n",
        "!pip install stable-audio-tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Step 3:"
      ],
      "metadata": {
        "id": "Tnazyrip9r6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown after restarting from the previous step, run this cell to install LoRAW and then it will require one more reestart.\n",
        "!git clone https://github.com/NeuralNotW0rk/LoRAW.git\n",
        "%cd LoRAW\n",
        "!pip install .\n",
        "%cd .."
      ],
      "metadata": {
        "cellView": "form",
        "id": "Sot4TwGn1ncR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Ri9Eskv9dHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Connect to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive connected.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h9SPKHL5dJHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Model and Configure settings"
      ],
      "metadata": {
        "id": "2JeGUv-19xlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Model from Hugging Face (Optionaly you can use your own uploaded model or one from your google drive)\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Hugging Face token\n",
        "# @markdown **IMPORTANT!** Make sure your Hugging Face token has the Permission to \"Read access to contents of all public gated repos you can access\" enabled in the Access Token Permissions if you are calling directly from the official Stable Audio base model repo.\n",
        "hf_token_text = \"\" # @param {type:\"string\"}\n",
        "download_dir = \"/content/base_model\" # @param {type:\"string\"}\n",
        "repo_name = \"stabilityai/stable-audio-open-1.0\" # @param {type:\"string\"}\n",
        "\n",
        "# Function to handle downloading the model from Hugging Face\n",
        "def handle_model_loading(hf_token, download_dir, repo_name):\n",
        "    if not hf_token:\n",
        "        print(\"Please provide your Hugging Face API token.\")\n",
        "        return\n",
        "\n",
        "    from huggingface_hub import hf_hub_download, HfApi\n",
        "\n",
        "    # Check if the token has the required permissions\n",
        "    try:\n",
        "        api = HfApi()\n",
        "        api.whoami(token=hf_token)\n",
        "    except Exception as e:\n",
        "        print(f\"Invalid or insufficient Hugging Face API token: {e}\")\n",
        "        return\n",
        "\n",
        "    # Create a directory for saving the downloaded files\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    # Download the model checkpoint\n",
        "    try:\n",
        "        model_file = hf_hub_download(\n",
        "            repo_id=repo_name,\n",
        "            filename=\"model.ckpt\",\n",
        "            use_auth_token=hf_token\n",
        "        )\n",
        "        print(f\"Downloaded model checkpoint to {model_file}\")\n",
        "\n",
        "        # Download the config file\n",
        "        config_file = hf_hub_download(\n",
        "            repo_id=repo_name,\n",
        "            filename=\"model_config.json\",\n",
        "            use_auth_token=hf_token\n",
        "        )\n",
        "        print(f\"Downloaded config file to {config_file}\")\n",
        "        print(f\"Copying to {download_dir}\")\n",
        "        # Copy the files to the desired directory\n",
        "        shutil.copy(model_file, os.path.join(download_dir, \"model.ckpt\"))\n",
        "        shutil.copy(config_file, os.path.join(download_dir, \"model_config.json\"))\n",
        "\n",
        "        print(f\"Copied model and config files to {download_dir}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading or copying files from Hugging Face: {e}\")\n",
        "\n",
        "# Call the function to handle the model loading\n",
        "handle_model_loading(hf_token_text, download_dir, repo_name)\n",
        "\n",
        "# @markdown When you are done here, the \"download_dir\" will be where your model.ckpt and model_config.ckpt are located\n"
      ],
      "metadata": {
        "id": "GUvMMj8pdIsJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Training Paths and Options\n",
        "# Hugging Face repository name for a Stable Audio Tools model\n",
        "# Will prioritize model.safetensors over model.ckpt in the repo\n",
        "# Optional, used in place of model-config and ckpt-path when using pre-trained model checkpoints on Hugging Face\n",
        "#@markdown Training Config\n",
        "\n",
        "#@markdown note: if you downloaded the model using the downloader with the default settings then these default paths will work for ckpt_path and model_config\n",
        "\n",
        "# Path to the model config file for a local model\n",
        "\n",
        "ckpt_path = \"/content/base_model/model.ckpt\" # @param {type:\"string\"}\n",
        "model_config = \"/content/base_model/model_config.json\" # @param {type:\"string\"}\n",
        "\n",
        "# Path to unwrapped model checkpoint file for a local model\n",
        "# pretrained_name = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Used in various model types such as latent diffusion models to load a pre-trained autoencoder.\n",
        "# Requires an unwrapped model checkpoint.\n",
        "pretransform_ckpt_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# The directory in which to save the model checkpoints\n",
        "save_dir = \"/trained\" # @param {type:\"string\"}\n",
        "\n",
        "# The number of steps between saved checkpoints.\n",
        "# Default: 10000\n",
        "checkpoint_every = 30 # @param {type:\"integer\"}\n",
        "\n",
        "# Number of samples per-GPU during training. Should be set as large as your GPU VRAM will allow.\n",
        "# Default: 8\n",
        "batch_size = 8 # @param {type:\"integer\"}\n",
        "\n",
        "# Enables and sets the number of batches for gradient batch accumulation. Useful for increasing effective batch size when training on smaller GPUs.\n",
        "accum_batches = 2 # @param {type:\"integer\"}\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-4 # @param {type:\"number\"}\n",
        "\n",
        "# Set to true to enable lora usage\n",
        "use_lora = True # @param {type:\"boolean\"}\n",
        "\n",
        "# A pre-trained lora continue from\n",
        "lora_ckpt_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# Enables ReLoRA training if set, the number of steps between full-rank updates\n",
        "relora_every = 1000 # @param {type:\"integer\"}\n",
        "# @markdown Quantize is currently broken\n",
        "# Set to true to enable 4-bit quantization of base model for QLoRA training\n",
        "quantize = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Advanced\n",
        "# Number of GPUs per-node to use for training\n",
        "# Default: 1\n",
        "# num_gpus = 1 # @param {type:\"integer\"}\n",
        "\n",
        "# Number of GPU nodes being used for training\n",
        "# Default: 1\n",
        "# num_nodes = 1 # @param {type:\"integer\"}\n",
        "\n",
        "# Multi-GPU strategy for distributed training. Setting to deepspeed will enable DeepSpeed ZeRO Stage 2.\n",
        "# Default: ddp if --num_gpus > 1, else None\n",
        "# strategy = \"ddp\" # @param {type:\"string\"}\n",
        "\n",
        "# Floating-point precision to use during training\n",
        "# Default: 16\n",
        "precision = 16 # @param {type:\"integer\"}\n",
        "\n",
        "# Number of CPU workers used by the data loader\n",
        "# num_workers = 8 # @param {type:\"integer\"}\n",
        "\n",
        "# RNG seed for PyTorch, helps with deterministic training\n",
        "# seed = -1 # @param {type:\"integer\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "e-rL_0j2dIh_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Update Parameters in model_config.json\n",
        "# @markdown Defaults are fine here, though sample_size is roughly 6.2 for training on free Colabs\n",
        "import json\n",
        "\n",
        "# Default parameters for lora\n",
        "sample_size = 276480 # @param {type:\"number\"}\n",
        "component_whitelist = [\"transformer\"] # @param {type:\"raw\"}\n",
        "multiplier = 1.0 # @param {type:\"number\"}\n",
        "rank = 16 # @param {type:\"integer\"}\n",
        "alpha = 16 # @param {type:\"integer\"}\n",
        "dropout = 0.0 # @param {type:\"number\"}\n",
        "module_dropout = 0.0 # @param {type:\"number\"}\n",
        "\n",
        "# Load the existing config\n",
        "with open(model_config, 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Add or update lora parameters\n",
        "lora_defaults = {\n",
        "    \"component_whitelist\": component_whitelist,\n",
        "    \"multiplier\": multiplier,\n",
        "    \"rank\": rank,\n",
        "    \"alpha\": alpha,\n",
        "    \"dropout\": dropout,\n",
        "    \"module_dropout\": module_dropout,\n",
        "    \"lr\": learning_rate\n",
        "}\n",
        "\n",
        "config[\"lora\"] = config.get(\"lora\", {})\n",
        "config[\"lora\"].update(lora_defaults)\n",
        "\n",
        "# Update sample size\n",
        "config[\"sample_size\"] = sample_size\n",
        "\n",
        "# Save the updated config\n",
        "with open(model_config, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(f\"Updated lora parameters and sample size in {model_config}\")\n"
      ],
      "metadata": {
        "id": "DdFijadLdIbO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Dataset Config files\n",
        "#@markdown This will create the dataset.json and metadata.py files required for training\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Dataset folder param\n",
        "dataset_folder = \"/content/drive/MyDrive/YOUR_DATASET_PATH\" # @param {type:\"string\"}\n",
        "\n",
        "# Random crop param\n",
        "random_crop = False # @param {type:\"boolean\"}\n",
        "#@markdown advanced users can edit this cell to enhance the metadata collection process\n",
        "dataset_configs_folder = \"/content/dataset_configs\"\n",
        "os.makedirs(dataset_configs_folder, exist_ok=True)\n",
        "\n",
        "# Create dataset config JSON\n",
        "dataset_config = {\n",
        "    \"dataset_type\": \"audio_dir\",\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"id\": \"fine_tune\",\n",
        "            \"path\": dataset_folder,\n",
        "            \"custom_metadata_module\": f\"{dataset_configs_folder}/metadata.py\"\n",
        "        }\n",
        "    ],\n",
        "    \"random_crop\": random_crop\n",
        "}\n",
        "\n",
        "dataset_config_path = os.path.join(dataset_configs_folder, \"dataset_config.json\")\n",
        "\n",
        "with open(dataset_config_path, \"w\") as f:\n",
        "    json.dump(dataset_config, f, indent=4)\n",
        "\n",
        "print(f\"Created dataset config file at: {dataset_config_path}\")\n",
        "\n",
        "# Define the content for metadata.py\n",
        "metadata_py_content = f\"\"\"\n",
        "def get_custom_metadata(info, audio):\n",
        "    # Remove the dataset folder path from the root of the path\n",
        "    relative_path = info['path'].replace(\"{dataset_folder}\", \"\")\n",
        "    # Split the remaining path into components and join them with spaces, reversing the order\n",
        "    prompt = \",\".join(reversed(relative_path.strip(\"/\").replace(\"-\", \" \").replace(\"_\", \" \").split(\"/\")))\n",
        "    return {{\"prompt\": prompt}}\n",
        "\"\"\"\n",
        "\n",
        "# Path to the metadata.py file\n",
        "metadata_py_path = os.path.join(dataset_configs_folder, \"metadata.py\")\n",
        "\n",
        "# Write the content to metadata.py\n",
        "with open(metadata_py_path, 'w') as file:\n",
        "    file.write(metadata_py_content)\n",
        "\n",
        "print(f\"Created metadata.py at {metadata_py_path}\")\n"
      ],
      "metadata": {
        "id": "6YpzUgFfnDdd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "I4hYdbIB91sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run Training Command\n",
        "# Set the paths\n",
        "loRAW_dir = \"/content/LoRAW\"\n",
        "dataset_config_path = dataset_configs_folder+\"/dataset_config.json\"\n",
        "model_config_path = model_config\n",
        "pretrained_ckpt_path = ckpt_path\n",
        "\n",
        "print(f\"Dataset config path: {dataset_config_path}\")\n",
        "print(f\"Model config path: {model_config_path}\")\n",
        "print(f\"Pretrained ckpt path: {pretrained_ckpt_path}\")\n",
        "# Additional flags\n",
        "use_lora = True  # @param {type:\"boolean\"}\n",
        "%cd {loRAW_dir}\n",
        "# Construct the training command\n",
        "training_command = f\"python train.py --dataset-config {dataset_config_path} --model-config {model_config_path} --pretrained-ckpt-path {pretrained_ckpt_path} --batch-size {batch_size} --checkpoint-every {checkpoint_every} --accum-batches {accum_batches}\"\n",
        "\n",
        "if use_lora:\n",
        "    training_command += \" --use-lora true\"\n",
        "\n",
        "# Print the command (for debugging purposes)\n",
        "print(f\"Training command: {training_command}\")\n",
        "\n",
        "# Run the training command\n",
        "!{training_command}"
      ],
      "metadata": {
        "id": "pXlTvMCGvWrt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use The Lora on Gradio"
      ],
      "metadata": {
        "id": "DWDZF-A-93TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown Gradio Settings:\n",
        "# If true, a publicly shareable link will be created for the Gradio demo\n",
        "# share = False # @param {type:\"boolean\"}\n",
        "\n",
        "# Used together to set a login for the Gradio demo\n",
        "username = \"guest\" # @param {type:\"string\"}\n",
        "password = \"guest\" # @param {type:\"string\"}\n",
        "# /content/LoRAW/lightning_logs/YOUR_CHECKPOINTS_FOLDER\n",
        "lora_checkpoint_path = \"/content/LoRAW/lightning_logs/YOUR_FINE_TUNE_ID_HERE/checkpoints\" # @param {type:\"string\"}\n",
        "# If true, the model weights to half-precision\n",
        "model_half = False # @param {type:\"boolean\"}\n",
        "\n",
        "loRAW_dir = \"/content/LoRAW\"\n",
        "\n",
        "%cd {loRAW_dir}\n",
        "# Construct the training command\n",
        "gradio_command = f\"python run_gradio.py --model-config {model_config_path} --ckpt-path {pretrained_ckpt_path} --username {username} --password {password} --lora-dir {lora_checkpoint_path}\"\n",
        "print(gradio_command)\n",
        "# Print the command (for debugging purposes)\n",
        "!{gradio_command}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Nl47oed_08sr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}